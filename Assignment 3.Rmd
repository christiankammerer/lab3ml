---
title: "Lab3 assignment 3"
output: html_document
date: "2024-12-15"
---


```{r}
library(kernlab)
set.seed(1234567890)

data(spam)
foo <- sample(nrow(spam))
spam <- spam[foo,]
tr <- spam[1:3000, ]
va <- spam[3001:3800, ]
trva <- spam[1:3800, ]
te <- spam[3801:4601, ] 

by <- 0.3
err_va <- NULL
for(i in seq(by,5,by)){
  filter <- ksvm(type~.,data=tr,kernel="rbfdot",kpar=list(sigma=0.05),C=i,scaled=FALSE)
  mailtype <- predict(filter,va[,-58])
  t <- table(mailtype,va[,58])
  err_va <-c(err_va,(t[1,2]+t[2,1])/sum(t))
}

```

```{r}
best_C <- which.min(err_va) * by
best_C
```
For the next part wee keep the best C for all filters


```{r}
filter0 <- ksvm(type~.,data=tr,kernel="rbfdot",kpar=list(sigma=0.05),C=which.min(err_va)*by,scaled=FALSE)
mailtype <- predict(filter0,va[,-58])
t <- table(mailtype,va[,58])
err0 <- (t[1,2]+t[2,1])/sum(t)
err0

filter1 <- ksvm(type~.,data=tr,kernel="rbfdot",kpar=list(sigma=0.05),C=which.min(err_va)*by,scaled=FALSE)
mailtype <- predict(filter1,te[,-58])
t <- table(mailtype,te[,58])
err1 <- (t[1,2]+t[2,1])/sum(t)
err1

filter2 <- ksvm(type~.,data=trva,kernel="rbfdot",kpar=list(sigma=0.05),C=which.min(err_va)*by,scaled=FALSE)
mailtype <- predict(filter2,te[,-58])
t <- table(mailtype,te[,58])
err2 <- (t[1,2]+t[2,1])/sum(t)
err2

filter3 <- ksvm(type~.,data=spam,kernel="rbfdot",kpar=list(sigma=0.05),C=which.min(err_va)*by,scaled=FALSE)
mailtype <- predict(filter3,te[,-58])
t <- table(mailtype,te[,58])
err3 <- (t[1,2]+t[2,1])/sum(t)
err3
```

Filter 0 :
Verify how well the SVM model with the best C performs on the validation set.
err0 helps validate the tuning of C.

Filter 1 :
Measure the generalization error of the model on unseen data (test set).
err1 shows into how well the model performs outside the validation set.

Filter 2 :
By using more data (training + validation), the model has a potentially stronger fit.
err2 tests whether this improved training size leads to better generalization on the test set.

Filter 3 :
Train the final model using all available data, maximizing the information for training.
Evaluate on the test set to observe the generalization error.

# Questions

# 1. Which filter do we return to the user ? filter0, filter1, filter2 or filter3? Why?
The filter to return to the user is filter3. It's the filter that uses all the spam dataset, it can improve the generalization of the model.

# 2. What is the estimate of the generalization error of the filter returned to the user? err0, err1, err2 or err3? Why?
The estimate of the generalization error of the filter returned to the user is err3, err3 = 0.008739076.
Since this filter is trained on the most data and uses the best C from cross-validation, it is expected to generalize better than Filter 0, Filter 1, or Filter 2.

err0: Measures error on the validation set (va), which was only used to tune C. It does not estimate the model's performance on unseen data.
err1: Measures error on the test set (te), but the model is trained only on 3000 training samples, not the full dataset.
err2: Measures error on the test set (te), but the model is trained on 3800 samples (trva), which is larger than filter1, but still smaller than the entire dataset.
err3: Measures error on the test set (te), and since filter3 is trained on all 4601 samples, this is the best and most reliable model for generalization.

err0 = 0.175, err1 = 0.1610487, err2 = 0.1573034, err3 = 0.008739076.
The err3 value is much smaller than the other errors, but this is due to the fact that it is trained on all the data, including the test set (te). While this gives it a lower error, it might not reflect "true" generalization error, but it is still the error reported for Filter 3, which is the model returned to the user.

# 3


```{r}
# Extract support vector indices, coefficients, and intercept
sv <- alphaindex(filter3)[[1]] # Support vector indices
co <- coef(filter3)[[1]]       # Coefficients for the support vectors
inte <- -b(filter3)            # Negative intercept

# Initialize a vector to store the decision values
k <- NULL

# Compute decision values for the first 10 points
for (i in 1:10) {
  k2 <- NULL # Initialize contributions for the current data point
  for (j in 1:length(sv)) {
    # Compute the RBF kernel value between point i and support vector j
    diff <- as.numeric(spam[i, -58] - spam[sv[j], -58]) # Difference between features
    rbf_kernel <- exp(-sum(diff^2) / (2 * 0.05^2))      # RBF kernel computation
    k2 <- c(k2, co[j] * rbf_kernel)                    # Contribution from support vector j
  }
  # Compute final decision value for data point i
  k <- c(k, sum(k2) + inte)
}

# Output the computed decision values
k

```

```{r}
# Compare the computed decision values with predict()
predict(filter3, spam[1:10, -58], type = "decision")
```

Positive value: The point is classified as belonging to the positive class.
Negative value: The point is classified as belonging to the negative class.